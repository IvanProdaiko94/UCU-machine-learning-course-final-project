{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Comment Classification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Anton Bilchuk\n",
    "- Ivan Prodaiko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukrainian Catholic Univercity, May 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATENTION: for running this model CUDA supported device is required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1804874, 45)\n",
      "Test shape :  (97320, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "             ...             article_id    rating  funny  wow  sad  likes  \\\n",
       "0            ...                   2006  rejected      0    0    0      0   \n",
       "1            ...                   2006  rejected      0    0    0      0   \n",
       "2            ...                   2006  rejected      0    0    0      0   \n",
       "3            ...                   2006  rejected      0    0    0      0   \n",
       "4            ...                   2006  rejected      0    0    0      1   \n",
       "\n",
       "   disagree  sexual_explicit  identity_annotator_count  \\\n",
       "0         0              0.0                         0   \n",
       "1         0              0.0                         0   \n",
       "2         0              0.0                         0   \n",
       "3         0              0.0                         0   \n",
       "4         0              0.0                         4   \n",
       "\n",
       "   toxicity_annotator_count  \n",
       "0                         4  \n",
       "1                         4  \n",
       "2                         4  \n",
       "3                         4  \n",
       "4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will start with Embedding. We have to extract all words from train and test datasets and find their vector forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All coments shape: (1902194, 2)\n"
     ]
    }
   ],
   "source": [
    "all_comments = pd.concat([train[['id','comment_text']], test], axis=0)\n",
    "print(\"All coments shape: \" + str(all_comments.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load some pretrained word2vec embedding matrix. We used 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset.\n",
    "Here you can use any pretrained dataset you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeddings = KeyedVectors.load_word2vec_format('./inputs/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace aren't to are not and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_in = open(\"./inputs/contraction_mapping.pickle\",\"rb\")\n",
    "contraction_mapping = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace aren't to are not and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuations. Replace _ and quotes to space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "punctuations_mapping = {\"_\":\" \", \"`\":\" \"}\n",
    "\n",
    "def clean_punctuations(text):\n",
    "    for p in punctuations_mapping:\n",
    "        text = text.replace(p, punctuations_mapping[p])    \n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, \" \" + p + \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swears mean the same, but can be written in different forms. We decided to replace all swears with one word. Bad one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "swear_words = np.load('./inputs/swear_words.npy')\n",
    "swear_replaces = []\n",
    "for swear in swear_words:\n",
    "    if swear[1:(len(swear)-1)] not in embeddings:\n",
    "        swear_replaces.append(swear)\n",
    "swear_replaces = '|'.join(swear_replaces)\n",
    "\n",
    "def replace_swears(text):\n",
    "    return re.sub(swear_replaces, ' ' + swear_words[0] + ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply all preprocessing to comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "1/4 Done\n",
      "2/4 Done\n",
      "3/4 Done\n",
      "4/4 Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Start\")\n",
    "all_comments['comment_text'] = all_comments['comment_text'].apply(lambda x: x.lower())\n",
    "print(\"1/4 Done\")\n",
    "all_comments['comment_text'] = all_comments['comment_text'].apply(clean_contractions)\n",
    "print(\"2/4 Done\")\n",
    "all_comments['comment_text'] = all_comments['comment_text'].apply(clean_punctuations)\n",
    "print(\"3/4 Done\")\n",
    "all_comments['comment_text'] = all_comments['comment_text'].apply(replace_swears)\n",
    "print(\"4/4 Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>this is so cool  .   it is like  ,       '    ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>thank you  !    !   this would make my life a ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>this is such an urgent design problem  ;   kud...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>is this something i will be able to install on...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers  .</td>\n",
       "      <td>0.893617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                       comment_text    target\n",
       "0  59848  this is so cool  .   it is like  ,       '    ...  0.000000\n",
       "1  59849  thank you  !    !   this would make my life a ...  0.000000\n",
       "2  59852  this is such an urgent design problem  ;   kud...  0.000000\n",
       "3  59855  is this something i will be able to install on...  0.000000\n",
       "4  59856           haha you guys are a bunch of losers  .    0.893617"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts = all_comments.iloc[:len(train),:]\n",
    "test_texts = all_comments.iloc[len(train):,:]\n",
    "\n",
    "train = pd.concat([train_texts,train[['target']]],axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = np.where(train['target'] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "train_dataset, validation_dataset = model_selection.train_test_split(train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to tokenize all words. They would be represented as a unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As not all sentences have the same count of word, but our model has static input size we added pad_text function that will add empty paddings to the sentances that do not have enough words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NUM_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "TOXICITY_COLUMN = 'target'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_dataset[TEXT_COLUMN])\n",
    "\n",
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddigns matrix would represent 300 dimention vector not from word but from tokenized word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIMENSION = 300\n",
    "embeddings_matrix = np.zeros((len(tokenizer.word_index) + 1,EMBEDDINGS_DIMENSION))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if word in embeddings.vocab:\n",
    "        embeddings_matrix[index] = embeddings[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare all data before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainig\n",
    "train_text = pad_text(train_dataset[TEXT_COLUMN], tokenizer)\n",
    "train_labels = train_dataset[TOXICITY_COLUMN]\n",
    "\n",
    "# validation step\n",
    "validate_text = pad_text(validation_dataset[TEXT_COLUMN], tokenizer)\n",
    "validate_labels = validation_dataset[TOXICITY_COLUMN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Our model was describe into the paper. There you can also find a diagam and described layers. In short, this is a RNN GRU clasification neural network. It also has an Convolutional 1D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 256, 300)     90366600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 256, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256, 128)     140544      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 255, 64)      16448       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 90,523,721\n",
      "Trainable params: 157,121\n",
      "Non-trainable params: 90,366,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Input, Dense, CuDNNGRU,concatenate, Bidirectional, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# (256x1) This is sentence with word indexes in tokenizer.\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# (256x300) Embedding layer would transform every word index into 300 dimension vector.\n",
    "embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                            EMBEDDINGS_DIMENSION,\n",
    "                            weights=[embeddings_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "x = embedding_layer(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)   \n",
    "x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "avg_pooling = GlobalAveragePooling1D()(x)\n",
    "max_pooling = GlobalMaxPooling1D()(x)     \n",
    "\n",
    "x = concatenate([avg_pooling, max_pooling])\n",
    "\n",
    "dense = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, dense)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We learn till model would stop increase accurancy.\n",
    "Max epoch count is 100. And batch size 1024. It is pretty big batch size, but we have a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/100\n",
      "1624386/1624386 [==============================] - 524s 323us/step - loss: 0.1536 - acc: 0.9430 - val_loss: 0.1293 - val_acc: 0.9499\n",
      "Epoch 2/100\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1320 - acc: 0.9484 - val_loss: 0.1242 - val_acc: 0.9513\n",
      "Epoch 3/100\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1268 - acc: 0.9501 - val_loss: 0.1204 - val_acc: 0.9528\n",
      "Epoch 4/100\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1235 - acc: 0.9512 - val_loss: 0.1189 - val_acc: 0.9529\n",
      "Epoch 5/100\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1210 - acc: 0.9520 - val_loss: 0.1182 - val_acc: 0.9530\n",
      "Epoch 6/100\n",
      "1624386/1624386 [==============================] - 512s 315us/step - loss: 0.1190 - acc: 0.9528 - val_loss: 0.1186 - val_acc: 0.9523\n",
      "Epoch 7/100\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1174 - acc: 0.9534 - val_loss: 0.1173 - val_acc: 0.9528\n",
      "Epoch 8/100\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1159 - acc: 0.9539 - val_loss: 0.1168 - val_acc: 0.9535\n",
      "Epoch 9/100\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1147 - acc: 0.9543 - val_loss: 0.1186 - val_acc: 0.9515\n",
      "Epoch 10/100\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1134 - acc: 0.9549 - val_loss: 0.1174 - val_acc: 0.9528\n",
      "Epoch 11/100\n",
      "1624386/1624386 [==============================] - 513s 316us/step - loss: 0.1122 - acc: 0.9552 - val_loss: 0.1170 - val_acc: 0.9537\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_text,\n",
    "    train_labels,\n",
    "    batch_size=1024,\n",
    "    epochs=100,\n",
    "    validation_data=(validate_text, validate_labels),\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model..\n",
      "Saving embeddings..\n",
      "Saving word tokenizer..\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model..\")\n",
    "model.save('gru.h5')\n",
    "\n",
    "print(\"Saving embeddings..\")\n",
    "pd.DataFrame(embeddings_matrix).to_csv('./embedding_matrix.gru.csv')\n",
    "\n",
    "print(\"Saving word tokenizer..\")\n",
    "with open('tokenizer.gru.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
